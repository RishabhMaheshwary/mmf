(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{107:function(e,t,n){"use strict";n.d(t,"a",(function(){return p})),n.d(t,"b",(function(){return b}));var a=n(0),r=n.n(a);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=r.a.createContext({}),d=function(e){var t=r.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=d(e.components);return r.a.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},u=r.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,o=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),p=d(n),u=a,b=p["".concat(o,".").concat(u)]||p[u]||m[u]||i;return n?r.a.createElement(b,l(l({ref:t},s),{},{components:n})):r.a.createElement(b,l({ref:t},s))}));function b(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=u;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var s=2;s<i;s++)o[s]=n[s];return r.a.createElement.apply(null,o)}return r.a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},68:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return o})),n.d(t,"metadata",(function(){return l})),n.d(t,"toc",(function(){return c})),n.d(t,"default",(function(){return d}));var a=n(3),r=n(8),i=(n(0),n(107)),o={id:"movie_mcan",sidebar_label:"Movie+MCAN (VQA 2020 Winner)",title:"MoViE+MCAN (VQA 2020 Challenge Winner)"},l={unversionedId:"projects/movie_mcan",id:"projects/movie_mcan",isDocsHomePage:!1,title:"MoViE+MCAN (VQA 2020 Challenge Winner)",description:"This is a tutorial for running the MoViE+MCAN model which won the VQA Challenge at CVPR 2020. The winning team comprised of Nguyen, D. K., Jiang, H., Goswami, V., Yu. L. & Chen, X. MoViE+MCAN model is derived from the following papers, and is released under the MMF. Please cite both these papers if you use the model or the grid features used to train this model in your work:",source:"@site/docs/projects/movie_mcan.md",slug:"/projects/movie_mcan",permalink:"/docs/projects/movie_mcan",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/projects/movie_mcan.md",version:"current",lastUpdatedBy:"Vedanuj Goswami",lastUpdatedAt:1595900772,sidebar_label:"Movie+MCAN (VQA 2020 Winner)",sidebar:"docs",previous:{title:"TextCaps: a Dataset for Image Captioning with Reading Comprehension",permalink:"/docs/projects/m4c_captioner"},next:{title:"UniT: Multimodal Multitask Learning with a Unified Transformer",permalink:"/docs/projects/unit"}},c=[{value:"Installation",id:"installation",children:[]},{value:"Data Setup",id:"data-setup",children:[]},{value:"Training and Evaluation",id:"training-and-evaluation",children:[]},{value:"Inference Prediction for Eval AI Submission",id:"inference-prediction-for-eval-ai-submission",children:[]},{value:"Pretrained model",id:"pretrained-model",children:[]}],s={toc:c};function d(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(i.b)("wrapper",Object(a.a)({},s,n,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"This is a tutorial for running the MoViE+MCAN model which won the VQA Challenge at CVPR 2020. The winning team comprised of Nguyen, D. K., Jiang, H., Goswami, V., Yu. L. & Chen, X. MoViE+MCAN model is derived from the following papers, and is released under the MMF. Please cite both these papers if you use the model or the grid features used to train this model in your work:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Nguyen, D. K., Goswami, V., & Chen, X. (2020). ",Object(i.b)("em",{parentName:"li"},"Revisiting Modulated Convolutions for Visual Counting and Beyond"),". arXiv preprint arXiv:2004.11883. (",Object(i.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2004.11883"},"arXiV"),")")),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},"@article{nguyen2020revisiting,\n  title={Revisiting Modulated Convolutions for Visual Counting and Beyond},\n  author={Nguyen, Duy-Kien and Goswami, Vedanuj and Chen, Xinlei},\n  journal={arXiv preprint arXiv:2004.11883},\n  year={2020}\n}\n")),Object(i.b)("p",null,"and"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Jiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., & Chen, X. (2020). ",Object(i.b)("em",{parentName:"li"},"In Defense of Grid Features for Visual Question Answering"),". arXiv preprint arXiv:2001.03615. (",Object(i.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2001.03615"},"arXiV"),")")),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre"},"@article{jiang2020defense,\n  title={In Defense of Grid Features for Visual Question Answering},\n  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},\n  journal={arXiv preprint arXiv:2001.03615},\n  year={2020}\n}\n")),Object(i.b)("h2",{id:"installation"},"Installation"),Object(i.b)("p",null,"Install MMF following the ",Object(i.b)("a",{parentName:"p",href:"https://mmf.sh/docs/getting_started/installation/"},"installation guide"),"."),Object(i.b)("h2",{id:"data-setup"},"Data Setup"),Object(i.b)("p",null,"Annotations and features for VQA2.0 and VisualGenome will be automatically downloaded. The grid image features were extracted using the models trained in this ",Object(i.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/grid-feats-vqa"},"repo"),". Other variants of features data available in that ",Object(i.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/grid-feats-vqa"},"repo")," can also be used."),Object(i.b)("h2",{id:"training-and-evaluation"},"Training and Evaluation"),Object(i.b)("p",null,"To train MoViE+MCAN model on the VQA2.0 + Visual Genome dataset, run:"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-bash"},"mmf_run config=projects/movie_mcan/configs/vqa2/defaults.yaml \\\n    model=movie_mcan \\\n    dataset=vqa2 \\\n    run_type=train\n")),Object(i.b)("p",null,"this will save the trained model ",Object(i.b)("inlineCode",{parentName:"p"},"movie_mcan_final.pth")," in your ",Object(i.b)("inlineCode",{parentName:"p"},"./save")," directory for the experiment."),Object(i.b)("p",null,"To evaluate the trained model on the VQA2.0 val set, run:"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-bash"},"mmf_run config=projects/movie_mcan/configs/vqa2/defaults.yaml \\\n    model=movie_mcan \\\n    dataset=vqa2 \\\n    run_type=val \\\n    checkpoint.resume_file=<path_to_trained_pth_file>\n")),Object(i.b)("h2",{id:"inference-prediction-for-eval-ai-submission"},"Inference Prediction for Eval AI Submission"),Object(i.b)("p",null,"To generate the vqa prediction file for Eval AI submission on ",Object(i.b)("inlineCode",{parentName:"p"},"test-dev"),", run:"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-bash"},"mmf_predict config=projects/movie_mcan/configs/vqa2/defaults.yaml \\\n    model=movie_mcan \\\n    dataset=vqa2 \\\n    run_type=test \\\n    checkpoint.resume_file=<path_to_trained_pth_file>\n")),Object(i.b)("h2",{id:"pretrained-model"},"Pretrained model"),Object(i.b)("table",null,Object(i.b)("thead",{parentName:"table"},Object(i.b)("tr",{parentName:"thead"},Object(i.b)("th",{parentName:"tr",align:null},"Datasets"),Object(i.b)("th",{parentName:"tr",align:null},"Config File"),Object(i.b)("th",{parentName:"tr",align:null},"Pretrained Model Key"),Object(i.b)("th",{parentName:"tr",align:null},"Metrics"),Object(i.b)("th",{parentName:"tr",align:null},"Notes"))),Object(i.b)("tbody",{parentName:"table"},Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",{parentName:"tr",align:null},"VQA2.0 (",Object(i.b)("inlineCode",{parentName:"td"},"vqa2"),")"),Object(i.b)("td",{parentName:"tr",align:null},Object(i.b)("inlineCode",{parentName:"td"},"projects/movie_mcan/configs/vqa2/defaults.yaml")),Object(i.b)("td",{parentName:"tr",align:null},Object(i.b)("inlineCode",{parentName:"td"},"movie_mcan.grid.vqa2_vg")),Object(i.b)("td",{parentName:"tr",align:null},"testdev accuracy - 73.92%"),Object(i.b)("td",{parentName:"tr",align:null},"Uses Visual Genome as extra data")))),Object(i.b)("p",null,"To generate predictions with the pretrained MoViE+MCAN model on VQA2.0 ",Object(i.b)("inlineCode",{parentName:"p"},"test-dev")," set (assuming that the pretrained model that you are evaluating is ",Object(i.b)("inlineCode",{parentName:"p"},"movie_mcan.grid.vqa2_vg"),"), run:"),Object(i.b)("pre",null,Object(i.b)("code",{parentName:"pre",className:"language-bash"},"mmf_predict config=projects/movie_mcan/configs/vqa2/defaults.yaml \\\n  dataset=vqa2 \\\n  model=movie_mcan \\\n  run_type=test \\\n  checkpoint.resume_zoo=movie_mcan.grid.vqa2_vg\n")),Object(i.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.b)("div",{parentName:"div",className:"admonition-heading"},Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",{parentName:"h5",className:"admonition-icon"},Object(i.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(i.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),Object(i.b)("div",{parentName:"div",className:"admonition-content"},Object(i.b)("p",{parentName:"div"},"Follow ",Object(i.b)("a",{parentName:"p",href:"https://mmf.sh/docs/tutorials/checkpointing"},"checkpointing")," tutorial to understand more fine-grained details of checkpoint, loading and resuming in MMF"))))}d.isMDXComponent=!0}}]);