(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{107:function(e,t,r){"use strict";r.d(t,"a",(function(){return b})),r.d(t,"b",(function(){return f}));var a=r(0),n=r.n(a);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function c(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var p=n.a.createContext({}),l=function(e){var t=n.a.useContext(p),r=t;return e&&(r="function"==typeof e?e(t):c(c({},t),e)),r},b=function(e){var t=l(e.components);return n.a.createElement(p.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},u=n.a.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),b=l(r),u=a,f=b["".concat(i,".").concat(u)]||b[u]||m[u]||o;return r?n.a.createElement(f,c(c({ref:t},p),{},{components:r})):n.a.createElement(f,c({ref:t},p))}));function f(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,i=new Array(o);i[0]=u;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:a,i[1]=c;for(var p=2;p<o;p++)i[p]=r[p];return n.a.createElement.apply(null,i)}return n.a.createElement.apply(null,r)}u.displayName="MDXCreateElement"},89:function(e,t,r){"use strict";r.r(t),r.d(t,"frontMatter",(function(){return i})),r.d(t,"metadata",(function(){return c})),r.d(t,"toc",(function(){return s})),r.d(t,"default",(function(){return l}));var a=r(3),n=r(8),o=(r(0),r(107)),i={id:"projects",title:"MMF Projects",sidebar_label:"MMF Projects"},c={unversionedId:"notes/projects",id:"notes/projects",isDocsHomePage:!1,title:"MMF Projects",description:"MMF contains references implementations or has been used to develop following projects (in no particular order):",source:"@site/docs/notes/projects.md",slug:"/notes/projects",permalink:"/docs/notes/projects",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/notes/projects.md",version:"current",lastUpdatedBy:"Amanpreet Singh",lastUpdatedAt:1591892152,sidebar_label:"MMF Projects",sidebar:"docs",previous:{title:"Pretrained Models",permalink:"/docs/notes/pretrained_models"},next:{title:"Adding a dataset",permalink:"/docs/tutorials/dataset"}},s=[],p={toc:s};function l(e){var t=e.components,r=Object(n.a)(e,["components"]);return Object(o.b)("wrapper",Object(a.a)({},p,r,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"MMF contains references implementations or has been used to develop following projects (in no particular order):"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1911.06258"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/m4c"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1908.02265"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/vilbert"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"Visualbert: A simple and performant baseline for vision and language [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1908.03557"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1908.03557"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2005.04790"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/hateful_memes"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"Towards VQA Models That Can Read [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1904.08920"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/lorra"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"TextCaps: a Dataset for Image Captioning with Reading Comprehension [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2003.12462"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/m4c_captioner"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"Pythia v0. 1: the winning entry to the vqa challenge 2018 [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1807.09956"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/pythia"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"Bottom-up and top-down attention for image captioning and visual question answering [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1707.07998"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/butd"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"Supervised Multimodal Bitransformers for Classifying Images and Text [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/1909.02950"},"arXiv"),"] [",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/mmbt"},"project"),"]"),Object(o.b)("li",{parentName:"ul"},"Are we pretraining it right? Digging deeper into visio-linguistic pretraining [",Object(o.b)("a",{parentName:"li",href:"https://arxiv.org/abs/2004.08744"},"arXiv"),"][",Object(o.b)("a",{parentName:"li",href:"https://github.com/facebookresearch/mmf/tree/master/projects/pretrain_vl_right"},"project"),"]")))}l.isMDXComponent=!0}}]);