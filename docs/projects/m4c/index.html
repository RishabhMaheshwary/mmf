<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="MMF Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="MMF Blog Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-135079836-3","auto"),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-135079836-3"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-135079836-3",{})</script><title data-react-helmet="true">Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA | MMF</title><meta data-react-helmet="true" property="og:image" content="https://mmf.sh/img/logo.png"><meta data-react-helmet="true" name="twitter:image" content="https://mmf.sh/img/logo.png"><meta data-react-helmet="true" name="twitter:image:alt" content="Image for MMF"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA | MMF"><meta data-react-helmet="true" name="description" content="This project page shows how to use M4C model from the following paper, released under the MMF:"><meta data-react-helmet="true" property="og:description" content="This project page shows how to use M4C model from the following paper, released under the MMF:"><meta data-react-helmet="true" property="og:url" content="https://mmf.sh/docs/projects/m4c"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.png"><link data-react-helmet="true" rel="canonical" href="https://mmf.sh/docs/projects/m4c"><link rel="stylesheet" href="/styles.5679a759.css">
<link rel="preload" href="/styles.01bd21e3.js" as="script">
<link rel="preload" href="/runtime~main.f70e28cc.js" as="script">
<link rel="preload" href="/main.e80ee451.js" as="script">
<link rel="preload" href="/1.49a5d1a1.js" as="script">
<link rel="preload" href="/2.5865849b.js" as="script">
<link rel="preload" href="/35.8acf92ac.js" as="script">
<link rel="preload" href="/36.2c70eb31.js" as="script">
<link rel="preload" href="/935f2afb.f05b7531.js" as="script">
<link rel="preload" href="/17896441.d81ac98d.js" as="script">
<link rel="preload" href="/dbd623a8.03aa0bd4.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script>
<div style="display: none; text-align: center; background-color: white; color: black;" id="internaldocs-banner"></div><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/img/banner_logo.svg" alt="MMF Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/banner_logo.svg" alt="MMF Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs">Docs</a><a class="navbar__item navbar__link" href="/api_redirect">API</a><a href="https://github.com/facebookresearch/mmf" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub</a></div><div class="navbar__items navbar__items--right"></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/img/banner_logo.svg" alt="MMF Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/banner_logo.svg" alt="MMF Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/docs">Docs</a></li><li class="menu__list-item"><a class="menu__link" href="/api_redirect">API</a></li><li class="menu__list-item"><a href="https://github.com/facebookresearch/mmf" target="_blank" rel="noopener noreferrer" class="menu__link">GitHub</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_vMrn"><div class="docSidebarContainer_3Ak5" role="complementary"><div class="sidebar_3gvy"><div class="menu menu--responsive thin-scrollbar menu_1yIk"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_1CUI" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menuLinkText_yu3-">Getting started</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/">Installation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting_started/features">MMF Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting_started/quickstart">Quickstart</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting_started/video_overview">Video overview</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting_started/faqs">FAQs</a></li></ul></li><li class="menu__list-item"><a class="menu__link menuLinkText_yu3-">Notes</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/concepts">Terminology and Concepts</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/configuration">Configuration System</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/training_tricks">Training Tricks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/dataset_zoo">Dataset Zoo</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/model_zoo">Model Zoo</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/pretrained_models">Pretrained Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/projects">MMF Projects</a></li></ul></li><li class="menu__list-item"><a class="menu__link menuLinkText_yu3-">Tutorials</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/dataset">Adding a dataset</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/concat_bert_tutorial">Adding a model - Concat BERT</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/image_feature_extraction">Image Feature Extraction</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/checkpointing">Checkpointing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/processors">Adding a Processor</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/tutorials/slurm">Sweeping on Slurm</a></li></ul></li><li class="menu__list-item"><a class="menu__link menuLinkText_yu3-">Challenges</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/challenges/hateful_memes_challenge">Hateful Memes Challenge</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/challenges/textvqa_challenge">TextVQA Challenge</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/challenges/vqa_challenge">VQA Challenge</a></li></ul></li><li class="menu__list-item"><a class="menu__link menuLinkText_yu3-">Projects</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/projects/butd">BUTD</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/projects/m4c">M4C</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/projects/m4c_captioner">M4C-Captioner</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/projects/movie_mcan">Movie+MCAN (VQA 2020 Winner)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/projects/unit">UniT</a></li></ul></li></ul></div></div></div><main class="docMainContainer_2iGs"><div class="container padding-vert--lg docItemWrapper_1bxp"><div class="row"><div class="col docItemCol_U38p"><div class="docItemContainer_a7m4"><article><header><h1 class="docTitle_Oumm">Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</h1></header><div class="markdown"><p>This project page shows how to use M4C model from the following paper, released under the MMF:</p><ul><li>R. Hu, A. Singh, T. Darrell, M. Rohrbach, <em>Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</em>. in CVPR, 2020 (<a href="https://arxiv.org/pdf/1911.06258.pdf" target="_blank" rel="noopener noreferrer">PDF</a>)</li></ul><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-undefined codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">@inproceedings{hu2020iterative,</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  title={Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  author={Hu, Ronghang and Singh, Amanpreet and Darrell, Trevor and Rohrbach, Marcus},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  year={2020}</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">}</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>Project Page: <a href="http://ronghanghu.com/m4c" target="_blank" rel="noopener noreferrer">http://ronghanghu.com/m4c</a></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="installation"></a>Installation<a class="hash-link" href="#installation" title="Direct link to heading">#</a></h2><p>Install MMF following the <a href="https://mmf.sh/docs/getting_started/installation/" target="_blank" rel="noopener noreferrer">installation guide</a>.</p><p>This will install all M4C dependencies such as <code>transformers</code> and <code>editdistance</code> and will also compile the python interface for PHOC features.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="notes-about-data"></a>Notes about data<a class="hash-link" href="#notes-about-data" title="Direct link to heading">#</a></h2><p>This repo supports training and evaluation of the M4C model under three datasets: TextVQA, ST-VQA, and OCR-VQA. As you run a command, these datasets and the requirements would be automatically downloaded for you.</p><p>For the ST-VQA dataset, we notice that many images from COCO-Text in the downloaded ST-VQA data (around 1/3 of all images) are resized to 256×256 for unknown reasons, which degrades the image quality and distorts their aspect ratios. In the released object and OCR features below, we replaced these images with their original versions from COCO-Text as inputs to object detection and OCR systems.</p><p>The released imdbs contain OCR results and normalized bounding boxes (i.e. in the range of <code>[0,1]</code>) of each detected objects (under <code>obj_normalized_boxes</code> key) and OCR tokens (under <code>ocr_normalized_boxes</code> key). Note that the answers in ST-VQA and OCR-VQA imdbs are tiled (duplicated) to 10 answers per question to make its format consistent with the TextVQA imdbs.</p><p>For the TextVQA dataset, the downloaded file contains both imdbs with the Rosetta-en OCRs (better performance) and imdbs with Rosetta-ml OCRs (same OCR results as in the previous <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">LoRRA</a> model). Please download the corresponding OCR feature files.</p><p>Note that the object Faster R-CNN features are extracted with <a href="https://github.com/facebookresearch/mmf/blob/master/tools/scripts/features/extract_features_vmb.py" target="_blank" rel="noopener noreferrer"><code>extract_features_vmb.py</code></a> and the OCR Faster R-CNN features are extracted with <a href="https://github.com/facebookresearch/mmf/blob/master/projects/m4c/scripts/extract_ocr_frcn_feature.py" target="_blank" rel="noopener noreferrer"><code>extract_ocr_frcn_feature.py</code></a>.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="pretrained-m4c-models"></a>Pretrained M4C Models<a class="hash-link" href="#pretrained-m4c-models" title="Direct link to heading">#</a></h2><p>We release the following pretrained models for M4C on three datasets: TextVQA, ST-VQA, and OCR-VQA.</p><p>For the TextVQA dataset, we release three versions: M4C trained with ST-VQA as additional data (our best model) with Rosetta-en, M4C trained on TextVQA alone with Rosetta-en, and M4C trained on TextVQA alone with Rosetta-ml (same OCR results as in the previous <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">LoRRA</a> model).</p><table><thead><tr><th>Datasets</th><th>Config Files (under <code>projects/m4c/configs</code>)</th><th>Pretrained Model Key</th><th>Metrics</th><th>Notes</th></tr></thead><tbody><tr><td>TextVQA (<code>textvqa</code>)</td><td><code>textvqa/join_with_stvqa.yaml</code></td><td><code>m4c.textvqa.with_stvqa</code></td><td>val accuracy - 40.55%; test accuracy - 40.46%</td><td>Rosetta-en OCRs; ST-VQA as additional data</td></tr><tr><td>TextVQA (<code>textvqa</code>)</td><td><code>textvqa/defaults.yaml</code></td><td><code>m4c.textvqa.alone</code></td><td>val accuracy - 39.40%; test accuracy - 39.01%</td><td>Rosetta-en OCRs</td></tr><tr><td>TextVQA (<code>textvqa</code>)</td><td><code>textvqa/ocr_ml.yaml</code></td><td>m4c.textvqa.ocr_ml</td><td>val accuracy - 37.06%</td><td>Rosetta-ml OCRs</td></tr><tr><td>ST-VQA (<code>stvqa</code>)</td><td><code>stvqa/defaults.yaml</code></td><td><code>m4c.stvqa.defaults</code></td><td>val ANLS - 0.472 (accuracy - 38.05%); test ANLS - 0.462</td><td>Rosetta-en OCRs</td></tr><tr><td>OCR-VQA (<code>ocrvqa</code>)</td><td><code>ocrvqa/defaults.yaml</code></td><td><code>m4c.ocrvqa.defaults</code></td><td>val accuracy - 63.52%; test accuracy - 63.87%</td><td>Rosetta-en OCRs</td></tr></tbody></table><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="training-and-evaluation"></a>Training and Evaluation<a class="hash-link" href="#training-and-evaluation" title="Direct link to heading">#</a></h2><p>Please follow the <a href="https://mmf.sh/docs/getting_started/quickstart#training" target="_blank" rel="noopener noreferrer">MMF documentation</a> for the training and evaluation of the M4C model on each dataset.</p><p>For example:</p><ol><li>to train the M4C model on the TextVQA training set:</li></ol><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-bash codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">mmf_run </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">dataset</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">textvqa </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">model</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">m4c </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">projects/m4c/configs/textvqa/defaults.yaml </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  env.save_dir</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">./save/m4c</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>(Replace <code>textvqa</code> with other datasets and <code>projects/m4c/configs/textvqa/defaults.yaml</code> with other config files to train with other datasets and configurations. See the table above. You can also specify a different path to <code>env.save_dir</code> to save to a location you prefer.)</p><ol start="2"><li>To evaluate the pretrained M4C model locally on the a TextVQA&#x27;s validation set (assuming that the pretrained model that you are evaluating is <code>m4c.textvqa.with_stvqa</code>):</li></ol><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-bash codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">mmf_run </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">dataset</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">textvqa </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">model</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">m4c </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">projects/m4c/configs/textvqa/defaults.yaml </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  env.save_dir</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">./save/m4c </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">run_type</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">val </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  checkpoint.resume_zoo</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">m4c.textvqa.with_stvqa</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>As with training, you can replace <code>dataset</code>, <code>config</code> and <code>checkpoint.resume_zoo</code> according to the setting you want to evaluate.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>Use <code>checkpoint.resume=True</code> AND <code>checkpoint.resume_best=True</code> instead of <code>checkpoint.resume_zoo=m4c.textvqa.with_stvqa</code> to evaluate your trained snapshots.</p></div></div><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p>Follow <a href="https://mmf.sh/docs/tutorials/checkpointing" target="_blank" rel="noopener noreferrer">checkpointing</a> tutorial to understand more fine-grained details of checkpoint, loading and resuming in MMF</p></div></div><ol start="3"><li>to generate the EvalAI prediction files for the TextVQA test set (assuming you are evaluating the pretrained model <code>m4c.textvqa.with_stvqa</code>):</li></ol><div class="mdxCodeBlock_1zKU"><div class="codeBlockContent_actS"><div tabindex="0" class="prism-code language-bash codeBlock_tuNs thin-scrollbar"><div class="codeBlockLines_3uvA" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">mmf_predict </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">dataset</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">textvqa </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">model</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">m4c </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">projects/m4c/configs/textvqa/defaults.yaml </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  env.save_dir</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">./save/m4c </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">run_type</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">test </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  checkpoint.resume_zoo</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">m4c.textvqa.with_stvqa</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_2GIj">Copy</button></div></div><p>As before, for generating prediction for other pretrained model for TextVQA, replace <code>config</code> and <code>checkpoint.resume_zoo</code> according to the setting you want in the table.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>To generate predictions on val set, use <code>run_type=val</code> instead of <code>run_type=test</code>. As before, to generate predictions for your checkpoint, use <code>checkpoint.resume=True</code> AND <code>checkpoint.resume_best=True</code> instead of <code>checkpoint.resume_zoo=m4c.textvqa.with_stvqa</code>.</p></div></div></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/facebookresearch/mmf/edit/master/website/docs/projects/m4c.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2LL7"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-09-10T16:34:14.000Z" class="docLastUpdatedAt_1Qna">9/10/2020</time> by <strong>Ronghang Hu</strong></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/projects/butd"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« BUTD</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/projects/m4c_captioner"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">TextCaps: a Dataset for Image Captioning with Reading Comprehension »</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_2xL- thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#installation" class="table-of-contents__link">Installation</a></li><li><a href="#notes-about-data" class="table-of-contents__link">Notes about data</a></li><li><a href="#pretrained-m4c-models" class="table-of-contents__link">Pretrained M4C Models</a></li><li><a href="#training-and-evaluation" class="table-of-contents__link">Training and Evaluation</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a href="https://medium.com/pytorch/bootstrapping-a-multimodal-project-using-mmf-a-pytorch-powered-multimodal-framework-464f75164af7" target="_blank" rel="noopener noreferrer" class="footer__link-item">Blog</a></li><li class="footer__item"><a href="https://github.com/facebookresearch/mmf" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Legal</h4><ul class="footer__items"><li class="footer__item"><a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener" class="footer__link-item">Privacy</a></li><li class="footer__item"><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener" class="footer__link-item">Terms</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://opensource.facebook.com" target="_blank" rel="noopener noreferrer" class="footerLogoLink_31Aa"><img class="footer__logo" alt="Facebook Open Source Logo" src="/img/oss_logo.png"></a></div><div class="footer__copyright">Copyright © 2021 Facebook, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/styles.01bd21e3.js"></script>
<script src="/runtime~main.f70e28cc.js"></script>
<script src="/main.e80ee451.js"></script>
<script src="/1.49a5d1a1.js"></script>
<script src="/2.5865849b.js"></script>
<script src="/35.8acf92ac.js"></script>
<script src="/36.2c70eb31.js"></script>
<script src="/935f2afb.f05b7531.js"></script>
<script src="/17896441.d81ac98d.js"></script>
<script src="/dbd623a8.03aa0bd4.js"></script>
</body>
</html>